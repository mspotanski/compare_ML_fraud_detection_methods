- Originally implemented Categorical NB Classifier on the Base fraud dataset with SMOTE oversampling and 5-fold Cross Validation
- Initial results showed near perfect results, but this was found to be an error caused by including the target value in the train data
- Removing the data produced accuracy of 85% and precision of 5% wiht a huge amount of False-Positive values
- This is due to what SMOTE does to the Prior Distribution for the Naive Bayes assumption
- Massively boosting the prior distribution means we make it more likely for something to be fraud than what the distribution says
- Removing the SMOTE oversampling fixed this, boosting accuracy to 97% and precision up to 15%, which still isn't great
- This was across all data samples and only in the base. Still compiling results over all other skewed datasets
- I think I've fit the max performance of a categorical model on the data, as oversampling is no longer a good option, and sub-setting doesn't seem promising
- I believe the greatest limitation to the model's ability to identify discrete trends due to the naive assumption. It can do well over count data, such as with Multinomial data, or purely continuous or discrete, but trying to apply a mix of variables to fit to a distribution proves to not be as optimal as some would thin. This is due to everything being probability based, and there are other algorithms that make less assumptions that will more than likely produce much more actionable and interpretable results (Decision trees, clustering, SVMs).
